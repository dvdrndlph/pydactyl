#!/usr/bin/env python
__author__ = 'David Randolph'
# Copyright (c) 2020-2021 David A. Randolph.
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without
# restriction, including without limitation the rights to use,
# copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following
# conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
import copy
import re
import sys
from pydactyl.dcorpus.DCorpus import DCorpus, DAnnotation
from pydactyl.dcorpus.PianoFingering import PianoFingering
from pydactyl.dcorpus.DEvaluation import DEvaluation, DEvalFunction
from pydactyl.dactyler.Parncutt import Parncutt, Jacobs, Badgerow, Balliauw
from pydactyl.dactyler.Parncutt import FINGER_SPANS, BALLIAUW_FINGER_SPANS, PhysicalRuler, ImaginaryBlackKeyRuler, Ruler
from pydactyl.dactyler.Random import Random
from pydactyl.eval.Corporeal import open_file, get_corpus, get_system_scores, \
     get_result_set, get_err_result_set, RANK_HEADINGS, ERR_HEADINGS, ERR_METHODS, WEIGHT_RE, STAFF

# da_corpus = DCorpus()
# da_corpus.append_from_db(client_id='695311d7e88d5f79b4945bf45d00cc77', selection_id='21')
# da_score = da_corpus.d_score_by_index(0)
# da_title = da_score.title()
# print(da_title)
# print(da_score)

FULL_CONTEXT = True

# ['ideal', 'badgerow', 'jacobs', 'parncutt', 'random']:
# ['ideal', 'jacobs', 'parncutt', 'random']:
MODELS = ['badgerow', 'badball', 'badpar', 'jacobs', 'parncutt', 'balliauw']
# MODELS = ['badgerow']
# MODELS = ['balliauw']
# MODELS = ['badball']


# CORPORA = ['full_american', 'parncutt_published']
CORPORA = ['parncutt_published']

#####################################################
# FUNCTIONS
#####################################################

#####################################################
# MAIN BLOCK
#####################################################

# FIXME: We should read the corpora from db or disk only once.
for model_name in MODELS:
    pivot_rpt_file = open_file(base_name='pivot_report', name=model_name, suffix='txt')
    rank_result_file = open_file(base_name='rank_result', name=model_name)
    err_result_file = open_file(base_name='err_result', name=model_name)
    mean_err_phrase_result_file = open_file(base_name='mean_err_phrase_result', name=model_name)
    weighted_mean_err_result_file = open_file(base_name='weighted_mean_err_result', name=model_name)
    mean_err_result_file = open_file(base_name='mean_err_result', name=model_name)
    rank_results = []
    err_results = []
    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer', 'pig', 'all_american', 'pure_american', 'parncutt_published', 'full_american']:
    # for corpus_name in ['pig', 'full_american', 'parncutt_published']:
    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer']:
    for corpus_name in CORPORA:
        da_corpus = get_corpus(corpus_name=corpus_name)
        for da_score in da_corpus.d_score_list():
            system_scores = get_system_scores(model_name=model_name, d_score=da_score)
            title = da_score.title()
            note_count = da_score.note_count(staff=STAFF)
            # print(da_score)
            print("{} in {}".format(title, corpus_name), file=pivot_rpt_file)
            abcdh = da_score.abcd_header()
            last_annot_id = abcdh.annotation_count()
            # print("last id: {}".format(last_annot_id))
            annot_id = 1
            for annot_id in range(1, last_annot_id+1):
                annot = abcdh.annotation_by_id(annot_id)
                comment = annot.comments()
                mat = re.match(WEIGHT_RE, comment)
                if mat:
                    weight = int(mat.group(1))
                else:
                    weight = 1

                human_score = copy.deepcopy(da_score)
                PianoFingering.finger_score(d_score=human_score, staff=STAFF, id=annot_id)

                evil = DEvaluation(human_score=human_score, system_scores=system_scores,
                                   staff=STAFF, full_context=FULL_CONTEXT)
                pvt_heading = "{} over {} {} human {}".format(model_name, corpus_name, title, annot_id)
                pivot_report = evil.pivot_count_report(heading=pvt_heading)
                print(pivot_report, file=pivot_rpt_file)

                for i in range(5):
                    rank = i + 1
                    result = get_result_set(evil, corpus_name=corpus_name, model_name=model_name,
                                            title=title, note_count=note_count,
                                            annot_id=annot_id, weight=weight, rank=rank)
                    rank_results.append(result)
                    evil.parameterize()  # Reset to defaults.
                err_result = get_err_result_set(evil, corpus_name=corpus_name, model_name=model_name,
                                                title=title, note_count=note_count,
                                                annot_id=annot_id, weight=weight)
                err_results.append(err_result)
    pivot_rpt_file.close()

    stdout_org = sys.stdout
    sys.stdout = rank_result_file

    # print(rank_results)
    for field in RANK_HEADINGS:
        print(field, end=',')
    print()
    for res in rank_results:
        for field in RANK_HEADINGS:
            print(res[field], end=',')
        print()
    rank_result_file.close()

    sys.stdout = err_result_file
    for field in ERR_HEADINGS:
        print(field, end=',')
    print()
    for res in err_results:
        for field in ERR_HEADINGS:
            if field in ['corpus', 'title']:
                print("{:<20s}".format(res[field]), end=',')
            elif field in ['model']:
                print("{:<8s}".format(res[field]), end=',')
            elif field in ['notes', 'ann_id', 'weight']:
                print(res[field], end=',')
                # print("{:>3d}".format(res[field]), end=',')
            else:
                print("{:7.5f}".format(float(res[field])), end=',')
        print()
    err_result_file.close()
    sys.stdout = stdout_org

    cmt_err = {}
    norm_err = {}
    corpus = ''
    title = ''
    model = ''

    for res in err_results:
        if res['corpus'] != corpus or res['model'] != model or res['title'] != title:
            corpus = res['corpus']
            model = res['model']
            title = res['title']

        if (corpus, model, title) not in cmt_err:
            cmt_err[(corpus, model, title)] = {'sums': {}, 'people': 0, 'notes': res['notes']}
            for method in ERR_METHODS:
                cmt_err[(corpus, model, title)]['sums'][method] = 0

        cmt_err[(corpus, model, title)]['people'] += res['weight']
        for meth in ERR_METHODS:
            cmt_err[(corpus, model, title)]['sums'][meth] += res[meth] * res['weight']

    headings = ['corpus', 'model', 'title', 'notes', 'people']
    headings.extend(ERR_METHODS)

    sys.stdout = mean_err_phrase_result_file
    for field in headings:
        print(field, end=',')
    print()
    note_total = {}
    phrase_count = {}
    # mean_err_phrase is the mean of all ERR measures assigned on a particular phrase across all people.
    # weighted_err_per_person aggregates all individual ERR scores weighted by the
    # length of the phrase used to generate each of them. If we divide this
    # aggregate by the sum total notes, we get the sum total ERR weighted by
    # phrase length. If we divide this by the total number of annotations, we get weighted_mean_err.
    # If we ignore the weighting by phrase length (notes) and treat each ERR measure the same, we get mean_err.
    # mean_err seems easier to deal with for statistical analysis.
    #
    weighted_err_per_person = {}
    err_per_person = {}
    for (corpus, model, title) in cmt_err:
        if (corpus, model) not in weighted_err_per_person:
            weighted_err_per_person[(corpus, model)] = {}
            err_per_person[(corpus, model)] = {}
            note_total[(corpus, model)] = 0
            phrase_count[(corpus, model)] = 0
        res = cmt_err[(corpus, model, title)]
        print("{:<20s}".format(corpus), end=',')
        print("{:<8s}".format(model), end=',')
        print("{:<20s}".format(title), end=',')
        print(res['notes'], end=',')
        print(res['people'], end=',')
        note_total[(corpus, model)] += res['notes']
        phrase_count[(corpus, model)] += 1

        for meth in ERR_METHODS:
            if meth not in weighted_err_per_person[(corpus, model)]:
                weighted_err_per_person[(corpus, model)][meth] = 0
                err_per_person[(corpus, model)][meth] = 0
            phrase_avg_err = float(res['sums'][meth])/res['people']
            print("{:7.5f}".format(phrase_avg_err), end=',')
            weighted_err_per_person[(corpus, model)][meth] += phrase_avg_err * res['notes']
            err_per_person[(corpus, model)][meth] += phrase_avg_err
        print()
    mean_err_phrase_result_file.close()
    sys.stdout = stdout_org

    headings = ['corpus', 'model']
    headings.extend(ERR_METHODS)

    sys.stdout = weighted_mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    for (corpus, model) in weighted_err_per_person:
        print("{}".format(corpus), end=',')
        print("{}".format(model), end=',')
        for method in weighted_err_per_person[(corpus, model)]:
            method_weighted_mean_err = weighted_err_per_person[(corpus, model)][method]
            method_weighted_mean_err /= note_total[(corpus, model)]
            print("{:7.5f}".format(method_weighted_mean_err), end=',')
        print()
    print()

    weighted_mean_err_result_file.close()
    sys.stdout = stdout_org

    sys.stdout = mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    for (corpus, model) in err_per_person:
        print("{}".format(corpus), end=',')
        print("{}".format(model), end=',')
        for method in err_per_person[(corpus, model)]:
            method_mean_err = err_per_person[(corpus, model)][method]
            method_mean_err /= phrase_count[(corpus, model)]
            print("{:7.5f}".format(method_mean_err), end=',')
        print()
    print()

    mean_err_result_file.close()
    sys.stdout = stdout_org
