#!/usr/bin/env python
__author__ = 'David Randolph'
# Copyright (c) 2020-2021 David A. Randolph.
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without
# restriction, including without limitation the rights to use,
# copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following
# conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
import sys
from pydactyl.eval.Corporeal import Corporeal, RANK_HEADINGS, ERR_HEADINGS, ERR_METHODS, WEIGHT_RE, STAFF

# da_corpus = DCorpus()
# da_corpus.append_from_db(client_id='695311d7e88d5f79b4945bf45d00cc77', selection_id='21')
# da_score = da_corpus.d_score_by_index(0)
# da_title = da_score.title()
# print(da_title)
# print(da_score)

VERSION = '0000'
FULL_CONTEXT = True

# ['ideal', 'badgerow', 'jacobs', 'parncutt', 'random']:
# ['ideal', 'jacobs', 'parncutt', 'random']:
# MODELS = ['jaball', 'jaSball', 'jaMball', 'badgerow', 'badball', 'badSball', 'badMball', 'badpar', 'jarules', 'jacobs', 'parncutt', 'balliauw']
MODELS = ['jaball', 'jaSball', 'jaMball']
# MODELS = ['badgerow']
# MODELS = ['balliauw']
# MODELS = ['badball']


# CORPORA = ['full_american', 'parncutt_published']
CORPORA = ['parncutt_published']
# CORPORA = ['layer_one']

#####################################################
# FUNCTIONS
#####################################################

#####################################################
# MAIN BLOCK
#####################################################
corp = Corporeal()
file_mode = 'a'

for model_name in MODELS:
    model = corp.get_model(model_name=model_name)
    rank_results = []
    err_results = []
    pivot_reports = {}

    pivot_rpt_file = corp.open_file(base_name='pivot_report', name=model_name, suffix='txt', mode=file_mode)
    rank_result_file = corp.open_file(base_name='rank_result', name=model_name, mode=file_mode)
    err_result_file = corp.open_file(base_name='err_result', name=model_name, mode=file_mode)
    mean_err_phrase_result_file = corp.open_file(base_name='mean_err_phrase_result', name=model_name, mode=file_mode)
    weighted_mean_err_result_file = corp.open_file(base_name='weighted_mean_err_result', name=model_name, mode=file_mode)
    mean_err_result_file = corp.open_file(base_name='mean_err_result', name=model_name, mode=file_mode)

    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer', 'pig', 'all_american', 'pure_american', 'parncutt_published', 'full_american']:
    # for corpus_name in ['pig', 'full_american', 'parncutt_published']:
    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer']:
    for corpus_name in CORPORA:
        er, pr, rr = corp.get_all_results(corpus_name=corpus_name, model=model, model_name=model_name,
                                          staff=STAFF, full_context=FULL_CONTEXT, version=VERSION)
        err_results.extend(er)
        pivot_reports = pivot_reports | pr
        rank_results.extend(rr)

    for (mod_name, corpus_name, title) in pivot_reports:
        if mod_name != model_name:
            continue
        print("{} in {}".format(title, corpus_name), file=pivot_rpt_file)
        annot_id = 1
        for pivot_report in pivot_reports[(model_name, corpus_name, title)]:
            pivot_heading = "{} over {} {} human {}".format(model_name, corpus_name, title, annot_id)
            print(pivot_report, file=pivot_rpt_file)
            annot_id += 1
    pivot_rpt_file.close()

    stdout_org = sys.stdout
    sys.stdout = rank_result_file

    for field in RANK_HEADINGS:
        print(field, end=',')
    print()
    for res in rank_results:
        for field in RANK_HEADINGS:
            print(res[field], end=',')
        print()
    rank_result_file.close()

    sys.stdout = err_result_file
    for field in ERR_HEADINGS:
        print(field, end=',')
    print()
    for res in err_results:
        for field in ERR_HEADINGS:
            if field in ['corpus', 'title']:
                print("{:<20s}".format(res[field]), end=',')
            elif field in ['model']:
                print("{:<8s}".format(res[field]), end=',')
            elif field in ['notes', 'ann_id', 'weight']:
                print(res[field], end=',')
                # print("{:>3d}".format(res[field]), end=',')
            else:
                print("{:7.5f}".format(float(res[field])), end=',')
        print()
    err_result_file.close()
    sys.stdout = stdout_org

    cmt_err = corp.get_cmt_err(err_results=err_results)
    per_person = corp.get_errs_per_person(cmt_err=cmt_err)
    headings = ['corpus', 'model', 'title', 'notes', 'people']
    headings.extend(ERR_METHODS)

    sys.stdout = mean_err_phrase_result_file
    for field in headings:
        print(field, end=',')
    print()
    # mean_err_phrase is the mean of all ERR measures assigned on a particular phrase across all people.
    # weighted_err_per_person aggregates all individual ERR scores weighted by the
    # length of the phrase used to generate each of them. If we divide this
    # aggregate by the sum total notes, we get the sum total ERR weighted by
    # phrase length. If we divide this by the total number of annotations, we get weighted_mean_err.
    # If we ignore the weighting by phrase length (notes) and treat each ERR measure the same, we get mean_err.
    # mean_err seems easier to deal with for statistical analysis.
    #
    weighted_err_per_person = per_person['weighted_err_per_person']
    err_per_person = per_person['err_per_person']
    note_total = per_person['note_total']
    phrase_count = per_person['phrase_count']

    for (corpus, mod_name, title) in cmt_err:
        res = cmt_err[(corpus, mod_name, title)]
        print("{:<20s}".format(corpus), end=',')
        print("{:<8s}".format(mod_name), end=',')
        print("{:<20s}".format(title), end=',')
        print(res['notes'], end=',')
        print(res['people'], end=',')
        for meth in ERR_METHODS:
            phrase_avg_err = float(res['sums'][meth])/res['people']
            print("{:7.5f}".format(phrase_avg_err), end=',')
        print()
    mean_err_phrase_result_file.close()
    sys.stdout = stdout_org

    headings = ['corpus', 'model']
    headings.extend(ERR_METHODS)

    sys.stdout = weighted_mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    prior_method = ''
    for (corpus, mod_name) in sorted(weighted_err_per_person):
        print("{},{}".format(corpus, mod_name), end=',')
        for method in weighted_err_per_person[(corpus, mod_name)]:
            method_weighted_mean_err = weighted_err_per_person[(corpus, mod_name)][method]
            print("{:7.5f}".format(method_weighted_mean_err), end=',')
        print()
    print()

    weighted_mean_err_result_file.close()
    sys.stdout = stdout_org

    sys.stdout = mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    for (corpus, mod_name) in sorted(err_per_person):
        print("{},{}".format(corpus, mod_name), end=',')
        for method in err_per_person[(corpus, mod_name)]:
            method_mean_err = err_per_person[(corpus, mod_name)][method]
            print("{:7.5f}".format(method_mean_err), end=',')
        print()
    print()

    mean_err_result_file.close()
    sys.stdout = stdout_org
