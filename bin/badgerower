#!/usr/bin/env python
__author__ = 'David Randolph'
# Copyright (c) 2020-2021 David A. Randolph.
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without
# restriction, including without limitation the rights to use,
# copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following
# conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
import copy
import re
import sys
from pydactyl.dcorpus.DCorpus import DCorpus, DAnnotation
from pydactyl.dcorpus.PianoFingering import PianoFingering
from pydactyl.dcorpus.DEvaluation import DEvaluation, DEvalFunction
from pydactyl.dactyler.Parncutt import Parncutt
from pydactyl.dactyler.Parncutt import Jacobs
from pydactyl.dactyler.Parncutt import Badgerow, FINGER_SPANS
from pydactyl.dactyler.Random import Random

# da_corpus = DCorpus()
# da_corpus.append_from_db(client_id='695311d7e88d5f79b4945bf45d00cc77', selection_id='21')
# da_score = da_corpus.d_score_by_index(0)
# da_title = da_score.title()
# print(da_title)
# print(da_score)


# ['ideal', 'badgerow', 'jacobs', 'parncutt', 'random']:
# ['ideal', 'jacobs', 'parncutt', 'random']:
# MODELS = ['badgerow', 'jacobs', 'parncutt']
MODELS = ['badgerow']

# CORPORA = ['full_american', 'parncutt_published']
CORPORA = ['parncutt_published']

STAFF = 'upper'
FULL_CONTEXT = True
OUTPUT_DIR = '/Users/dave/tb2/doc/data/badgerow'
PIG_DIR = '/Users/dave/tb2/didactyl/dd/corpora/pig/PianoFingeringDataset_v1.00/abcd/'
BERINGER_DIR = '/Users/dave/tb2/didactyl/dd/corpora/beringer/'
SCALES_DIR = BERINGER_DIR + 'scales'
ARPEGGIOS_DIR = BERINGER_DIR + 'arpeggios'
BROKEN_DIR = BERINGER_DIR + 'broken_chords'

QUERY = dict()

QUERY['full_american'] = '''
    select f.upper_staff as fingering,
           count(*) as weight,
           'Various Didactyl' as 'authority',
           'Pydactyl' as 'transcriber'
      from finger f
     inner join parncutt p
        on f.exercise = p.exercise
     where f.exercise = {}
       and f.upper_staff is not null
       and length(f.upper_staff) = p.length_full
     group by f.upper_staff
     order by weight desc'''

QUERY['all_american'] = '''
    select parncutt_fingering as fingering,
           total as weight,
           'Various Didactyl' as 'authority',
           'Pydactyl' as 'transcriber'
      from parncutt_binary
     where exercise = {}
     order by weight desc'''

QUERY['pure_american'] = '''
    select parncutt as fingering,
           'Various Didactyl' as 'authority',
           'Pydactyl' as 'transcriber',
           count(*) as weight
      from parncutt_american_pure
     where exercise = {} 
       and Advised = 'No'
     group by parncutt
     order by weight desc'''

QUERY['parncutt_published'] = '''
    select fingering,
           'Various Didactyl' as 'authority',
           'Pydactyl' as 'transcriber',
           subject_count as weight
      from parncutt_published
     where exercise = {}
     order by weight desc'''

FULL_ABC_QUERY = '''
    select exercise as piece_id,
           abc_full as abc_str
      from parncutt
     order by exercise'''

FRAGMENT_ABC_QUERY = '''
    select exercise as piece_id,
           abc_fragment as abc_str
      from parncutt
     order by exercise'''

LAYER_ONE_QUERY = '''
    select id as piece_id,
            abc_full as abc_str
      from selection_detail
     where id like 'c%1';
'''

RANK_HEADINGS = ['corpus', 'model', 'title', 'notes', 'ann_id', 'weight', 'rank', 'hmg', 'norm_hmg',
                 'al', 'norm_al',
                 'rho_no_d', 'rho_uni_d',
                 'p_sat', 'tri_p_sat', 'nua_p_sat', 'rlx_p_sat',
                 'tri_D', 'nua_D', 'rlx_D',
                 'norm_tri_D', 'norm_nua_D', 'norm_rlx_D']
ERR_METHODS = ['hmg', 'hmg_rho', 'al', 'al_rho', 'clvrst',
               'tri', 'tri_rho', 'tri_nua', 'tri_nua_rho', 'tri_rlx', 'tri_rlx_rho', 'tri_clvrst']
ERR_HEADINGS = ['corpus', 'model', 'title', 'notes', 'ann_id', 'weight']
ERR_HEADINGS.extend(ERR_METHODS)

WEIGHT_RE = r"^Weight:\s*(\d+)$"


#####################################################
# FUNCTIONS
#####################################################
def get_corpus(corpus_name):
    if corpus_name == 'pig':
        the_corpus = DCorpus()
        the_corpus.append_dir(corpus_dir=PIG_DIR, split_header_extension='abcd')
        return the_corpus

    if corpus_name == 'scales':
        the_corpus = DCorpus()
        the_corpus.append_dir(corpus_dir=SCALES_DIR)
        return the_corpus

    if corpus_name == 'arpeggios':
        the_corpus = DCorpus()
        the_corpus.append_dir(corpus_dir=ARPEGGIOS_DIR)
        return the_corpus

    if corpus_name == 'broken':
        the_corpus = DCorpus()
        the_corpus.append_dir(corpus_dir=BROKEN_DIR)
        return the_corpus

    if corpus_name == 'beringer':
        the_corpus = DCorpus()
        for subdir in [SCALES_DIR, ARPEGGIOS_DIR, BROKEN_DIR]:
            the_corpus.append_dir(corpus_dir=subdir)
        return the_corpus

    if corpus_name == 'layer_one':
        the_corpus = DCorpus()
        piece_query = LAYER_ONE_QUERY
        the_corpus.assemble_and_append_from_db(db='diii', port=3307, user='didactyl', passwd='Pha_Lang35',
                                               piece_query=piece_query, fingering_query=QUERY[corpus_name])
        return the_corpus

    piece_query = FRAGMENT_ABC_QUERY
    if corpus_name == 'full_american':
        piece_query = FULL_ABC_QUERY
    the_corpus = DCorpus()
    the_corpus.assemble_and_append_from_db(piece_query=piece_query,
                                           fingering_query=QUERY[corpus_name])
    return the_corpus


def get_system_scores(model_name, d_score):
    k = 5
    if model_name == 'ideal':
        return DEvaluation.get_best_pseudo_model_scores(d_score=da_score, staff=STAFF)
    else:
        d_corpus = DCorpus()
        d_corpus.append(d_score=d_score)
        if model_name == 'random':
            model = Random()
        elif model_name == 'parncutt':
            model = Parncutt()
        elif model_name == 'jacobs':
            model = Jacobs()
        elif model_name == 'badgerow':
            model = Badgerow(finger_spans=FINGER_SPANS)
        else:
            raise Exception("Bad model")

        model.load_corpus(d_corpus=d_corpus)
        advice = model.generate_advice(staff=STAFF, score_index=0, k=k)
        print(advice)

        sys_scores = []
        for r in (range(k)):
            sys_score = copy.deepcopy(da_score)
            sys_score.remove_annotations()
            abcdf = advice[0][r]
            ann = DAnnotation(abcdf=abcdf, authority=model_name)
            sys_score.annotate(d_annotation=ann)
            PianoFingering.finger_score(d_score=sys_score, staff=STAFF)
            sys_scores.append(sys_score)
        return sys_scores


def get_err_result_set(evil, corpus_name, model_name, title, note_count, annot_id, weight):
    err_result = {}

    err_result['corpus'] = corpus_name
    err_result['model'] = model_name
    err_result['title'] = title
    err_result['notes'] = note_count
    err_result['ann_id'] = annot_id
    err_result['weight'] = weight

    err_result['hmg'] = evil.expected_reciprocal_rank()
    evil.delta_function(DEvalFunction.delta_adjacent_long)
    err_result['al'] = evil.expected_reciprocal_rank()
    evil.mu_function(DEvalFunction.mu_scale)
    err_result['al_rho'] = evil.expected_reciprocal_rank()
    evil.delta_function(DEvalFunction.delta_hamming)
    err_result['hmg_rho'] = evil.expected_reciprocal_rank()

    evil.mu_function(None)
    err_result['tri'] = evil.expected_reciprocal_rank(trigram=True)
    evil.tau_function(DEvalFunction.tau_nuanced)
    err_result['tri_nua'] = evil.expected_reciprocal_rank(trigram=True)
    evil.tau_function(DEvalFunction.tau_relaxed)
    err_result['tri_rlx'] = evil.expected_reciprocal_rank(trigram=True)

    evil.mu_function(DEvalFunction.mu_scale)
    err_result['tri_rlx_rho'] = evil.expected_reciprocal_rank(trigram=True)
    evil.tau_function(DEvalFunction.tau_nuanced)
    err_result['tri_nua_rho'] = evil.expected_reciprocal_rank(trigram=True)
    evil.tau_function(DEvalFunction.tau_trigram)
    err_result['tri_rho'] = evil.expected_reciprocal_rank(trigram=True)

    evil.parameterize(delta_function=DEvalFunction.delta_adjacent_long, tau_function=DEvalFunction.tau_relaxed,
                      decay_function=DEvalFunction.decay_uniform,
                      mu_function=DEvalFunction.mu_scale, rho_decay_function=DEvalFunction.decay_uniform)
    err_result['clvrst'] = evil.expected_reciprocal_rank()
    err_result['tri_clvrst'] = evil.expected_reciprocal_rank(trigram=True)
    return err_result


def get_result_set(evil, corpus_name, model_name, title, note_count, annot_id, weight, rank):
    result = dict()

    result['corpus'] = corpus_name
    result['model'] = model_name
    result['title'] = title
    result['notes'] = note_count
    result['ann_id'] = annot_id
    result['weight'] = weight
    result['rank'] = rank

    result['hmg'] = evil.big_delta_at_rank(rank=rank)
    result['norm_hmg'] = evil.big_delta_at_rank(rank=rank, normalized=True)
    result['rho_no_d'] = evil.pivot_clashes_at_rank(rank=rank)
    result['p_sat'] = evil.prob_satisfied(rank=rank)

    evil.delta_function(DEvalFunction.delta_adjacent_long)
    result['al'] = evil.big_delta_at_rank(rank=rank)
    result['norm_al'] = evil.big_delta_at_rank(rank=rank, normalized=True)

    result['tri_D'] = evil.trigram_big_delta_at_rank(rank=rank)
    result['norm_tri_D'] = evil.trigram_big_delta_at_rank(rank=rank, normalized=True)
    result['tri_p_sat'] = evil.trigram_prob_satisfied(rank=rank)

    evil.tau_function(DEvalFunction.tau_nuanced)
    result['nua_D'] = evil.trigram_big_delta_at_rank(rank=rank)
    result['norm_nua_D'] = evil.trigram_big_delta_at_rank(rank=rank, normalized=True)
    result['nua_p_sat'] = evil.trigram_prob_satisfied(rank=rank)

    evil.tau_function(DEvalFunction.tau_relaxed)
    result['rlx_D'] = evil.trigram_big_delta_at_rank(rank=rank)
    result['norm_rlx_D'] = evil.trigram_big_delta_at_rank(rank=rank, normalized=True)
    result['rlx_p_sat'] = evil.trigram_prob_satisfied(rank=rank)

    evil.rho_decay_function(DEvalFunction.decay_uniform)
    result['rho_uni_d'] = evil.rho_at_rank(rank=rank)
    return result


def full_file_path(base_name, name, output_dir=OUTPUT_DIR, suffix='csv'):
    full_path = "{}/{}_{}.{}".format(output_dir, base_name, name, suffix)
    return full_path


def open_file(base_name, name, output_dir=OUTPUT_DIR, suffix='csv', mode='w'):
    full_path = full_file_path(base_name, name, output_dir, suffix)
    file = open(full_path, mode)
    return file

#####################################################
# MAIN BLOCK
#####################################################

# FIXME: We should read the corpora from db or disk only once.
for model_name in MODELS:
    pivot_rpt_file = open_file(base_name='pivot_report', name=model_name, suffix='txt')
    rank_result_file = open_file(base_name='rank_result', name=model_name)
    err_result_file = open_file(base_name='err_result', name=model_name)
    mean_err_phrase_result_file = open_file(base_name='mean_err_phrase_result', name=model_name)
    weighted_mean_err_result_file = open_file(base_name='weighted_mean_err_result', name=model_name)
    mean_err_result_file = open_file(base_name='mean_err_result', name=model_name)
    rank_results = []
    err_results = []
    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer', 'pig', 'all_american', 'pure_american', 'parncutt_published', 'full_american']:
    # for corpus_name in ['pig', 'full_american', 'parncutt_published']:
    # for corpus_name in ['scales', 'arpeggios', 'broken', 'beringer']:
    for corpus_name in CORPORA:
        da_corpus = get_corpus(corpus_name=corpus_name)
        for da_score in da_corpus.d_score_list():
            system_scores = get_system_scores(model_name=model_name, d_score=da_score)
            title = da_score.title()
            note_count = da_score.note_count(staff=STAFF)
            # print(da_score)
            print("{} in {}".format(title, corpus_name), file=pivot_rpt_file)
            abcdh = da_score.abcd_header()
            last_annot_id = abcdh.annotation_count()
            # print("last id: {}".format(last_annot_id))
            annot_id = 1
            for annot_id in range(1, last_annot_id+1):
                annot = abcdh.annotation_by_id(annot_id)
                comment = annot.comments()
                mat = re.match(WEIGHT_RE, comment)
                if mat:
                    weight = int(mat.group(1))
                else:
                    weight = 1

                human_score = copy.deepcopy(da_score)
                PianoFingering.finger_score(d_score=human_score, staff=STAFF, id=annot_id)

                evil = DEvaluation(human_score=human_score, system_scores=system_scores,
                                   staff=STAFF, full_context=FULL_CONTEXT)
                pvt_heading = "{} over {} {} human {}".format(model_name, corpus_name, title, annot_id)
                pivot_report = evil.pivot_count_report(heading=pvt_heading)
                print(pivot_report, file=pivot_rpt_file)

                for i in range(5):
                    rank = i + 1
                    result = get_result_set(evil, corpus_name=corpus_name, model_name=model_name,
                                            title=title, note_count=note_count,
                                            annot_id=annot_id, weight=weight, rank=rank)
                    rank_results.append(result)
                    evil.parameterize()  # Reset to defaults.
                err_result = get_err_result_set(evil, corpus_name=corpus_name, model_name=model_name,
                                                title=title, note_count=note_count,
                                                annot_id=annot_id, weight=weight)
                err_results.append(err_result)
    pivot_rpt_file.close()

    stdout_org = sys.stdout
    sys.stdout = rank_result_file

    # print(rank_results)
    for field in RANK_HEADINGS:
        print(field, end=',')
    print()
    for res in rank_results:
        for field in RANK_HEADINGS:
            print(res[field], end=',')
        print()
    rank_result_file.close()

    sys.stdout = err_result_file
    for field in ERR_HEADINGS:
        print(field, end=',')
    print()
    for res in err_results:
        for field in ERR_HEADINGS:
            if field in ['corpus', 'title']:
                print("{:<20s}".format(res[field]), end=',')
            elif field in ['model']:
                print("{:<8s}".format(res[field]), end=',')
            elif field in ['notes', 'ann_id', 'weight']:
                print(res[field], end=',')
                # print("{:>3d}".format(res[field]), end=',')
            else:
                print("{:7.5f}".format(float(res[field])), end=',')
        print()
    err_result_file.close()
    sys.stdout = stdout_org

    cmt_err = {}
    norm_err = {}
    corpus = ''
    title = ''
    model = ''

    for res in err_results:
        if res['corpus'] != corpus or res['model'] != model or res['title'] != title:
            corpus = res['corpus']
            model = res['model']
            title = res['title']

        if (corpus, model, title) not in cmt_err:
            cmt_err[(corpus, model, title)] = {'sums': {}, 'people': 0, 'notes': res['notes']}
            for method in ERR_METHODS:
                cmt_err[(corpus, model, title)]['sums'][method] = 0

        cmt_err[(corpus, model, title)]['people'] += res['weight']
        for meth in ERR_METHODS:
            cmt_err[(corpus, model, title)]['sums'][meth] += res[meth] * res['weight']

    headings = ['corpus', 'model', 'title', 'notes', 'people']
    headings.extend(ERR_METHODS)

    sys.stdout = mean_err_phrase_result_file
    for field in headings:
        print(field, end=',')
    print()
    note_total = {}
    phrase_count = {}
    # mean_err_phrase is the mean of all ERR measures assigned on a particular phrase across all people.
    # weighted_err_per_person aggregates all individual ERR scores weighted by the
    # length of the phrase used to generate each of them. If we divide this
    # aggregate by the sum total notes, we get the sum total ERR weighted by
    # phrase length. If we divide this by the total number of annotations, we get weighted_mean_err.
    # If we ignore the weighting by phrase length (notes) and treat each ERR measure the same, we get mean_err.
    # mean_err seems easier to deal with for statistical analysis.
    #
    weighted_err_per_person = {}
    err_per_person = {}
    for (corpus, model, title) in cmt_err:
        if (corpus, model) not in weighted_err_per_person:
            weighted_err_per_person[(corpus, model)] = {}
            err_per_person[(corpus, model)] = {}
            note_total[(corpus, model)] = 0
            phrase_count[(corpus, model)] = 0
        res = cmt_err[(corpus, model, title)]
        print("{:<20s}".format(corpus), end=',')
        print("{:<8s}".format(model), end=',')
        print("{:<20s}".format(title), end=',')
        print(res['notes'], end=',')
        print(res['people'], end=',')
        note_total[(corpus, model)] += res['notes']
        phrase_count[(corpus, model)] += 1

        for meth in ERR_METHODS:
            if meth not in weighted_err_per_person[(corpus, model)]:
                weighted_err_per_person[(corpus, model)][meth] = 0
                err_per_person[(corpus, model)][meth] = 0
            phrase_avg_err = float(res['sums'][meth])/res['people']
            print("{:7.5f}".format(phrase_avg_err), end=',')
            weighted_err_per_person[(corpus, model)][meth] += phrase_avg_err * res['notes']
            err_per_person[(corpus, model)][meth] += phrase_avg_err
        print()
    mean_err_phrase_result_file.close()
    sys.stdout = stdout_org

    headings = ['corpus', 'model']
    headings.extend(ERR_METHODS)

    sys.stdout = weighted_mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    for (corpus, model) in weighted_err_per_person:
        print("{}".format(corpus), end=',')
        print("{}".format(model), end=',')
        for method in weighted_err_per_person[(corpus, model)]:
            method_weighted_mean_err = weighted_err_per_person[(corpus, model)][method]
            method_weighted_mean_err /= note_total[(corpus, model)]
            print("{:7.5f}".format(method_weighted_mean_err), end=',')
        print()
    print()

    weighted_mean_err_result_file.close()
    sys.stdout = stdout_org

    sys.stdout = mean_err_result_file

    for field in headings:
        print(field, end=',')
    print()
    for (corpus, model) in err_per_person:
        print("{}".format(corpus), end=',')
        print("{}".format(model), end=',')
        for method in err_per_person[(corpus, model)]:
            method_mean_err = err_per_person[(corpus, model)][method]
            method_mean_err /= phrase_count[(corpus, model)]
            print("{:7.5f}".format(method_mean_err), end=',')
        print()
    print()

    mean_err_result_file.close()
    sys.stdout = stdout_org
